{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A script for collecting recall, precision, dice, iou, and difference maps\n",
    "\n",
    "Can be used for finetuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micro_sam.evaluation.evaluation import run_evaluation\n",
    "import os\n",
    "import tifffile as tiff\n",
    "import matplotlib.pyplot as plt\n",
    "from natsort import natsorted\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from utils.stack_manipulation_utils import save_indv_images\n",
    "from utils.microsam_utils import load_filepaths\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load finetuned dataset paths\n",
    "finetuned_dir = '/vol/biomedic3/bglocker/mscproj24/nma23/data/testing_directory/multi_model/finetuning'\n",
    "train_image_paths = load_filepaths(finetuned_dir, 'train_image_paths.pkl')\n",
    "train_gt_paths = load_filepaths(finetuned_dir, 'train_gt_paths.pkl')\n",
    "val_image_paths = load_filepaths(finetuned_dir, 'val_image_paths.pkl')\n",
    "val_gt_paths = load_filepaths(finetuned_dir, 'val_gt_paths.pkl')\n",
    "test_image_paths = load_filepaths(finetuned_dir, 'test_image_paths.pkl')\n",
    "test_gt_paths = load_filepaths(finetuned_dir, 'test_gt_paths.pkl')\n",
    "\n",
    "# Load finetuned normalised mask paths\n",
    "# image_files = natsorted(test_image_paths) \n",
    "\n",
    "test_gt_normalised_mask_dir = '/vol/biomedic3/bglocker/mscproj24/nma23/data/testing_directory/multi_model/eval_test_data/normalised_test_labels'\n",
    "# test_pred_normalised_mask_dir = \"/vol/biomedic3/bglocker/mscproj24/nma23/data/testing_directory/multi_model/finetuning/inference/v4_all_models_full_inference/vit_l/v4_eval/pred_binary\"\n",
    "\n",
    "gt_relative_filepaths = natsorted(os.listdir(test_gt_normalised_mask_dir))\n",
    "\n",
    "\n",
    "gt_abs_files = natsorted(os.listdir(test_gt_normalised_mask_dir))\n",
    "# pred_abs_files = natsorted(os.listdir(test_pred_normalised_mask_dir))\n",
    "\n",
    "gt_files = [os.path.join(test_gt_normalised_mask_dir, f) for f in gt_abs_files]\n",
    "# pred_files = [os.path.join(test_pred_normalised_mask_dir, f) for f in pred_abs_files]\n",
    "image_files = natsorted(test_image_paths)\n",
    "\n",
    "# assert len(gt_files) == len(pred_files) == len(image_files), \"Mismatch in the number of files.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def list_files_in_directory(directory_path):\n",
    "    file_list = []\n",
    "    for file in natsorted(os.listdir(directory_path)):\n",
    "        if file == 'Mask_Stack.tiff': # Skip full mask stack\n",
    "            continue\n",
    "        file_path = os.path.join(directory_path, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            file_list.append(file_path)\n",
    "    \n",
    "    return file_list\n",
    "\n",
    "def plot_samples(gt, prediction):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    axs[0].imshow(gt, cmap='gray')\n",
    "    axs[0].set_title('Test Label')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    axs[1].imshow(prediction, cmap='gray')\n",
    "    axs[1].set_title('Prediction Segmentation')\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_four_images(image, gt, phantast_pred, model_pred, model_name):\n",
    "\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(24, 6))\n",
    "\n",
    "    axs[0].imshow(image, cmap='gray')\n",
    "    axs[0].set_title('Brightfield Image')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    axs[1].imshow(gt, cmap='gray')\n",
    "    axs[1].set_title('Ground Truth')\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    axs[2].imshow(phantast_pred, cmap='gray')\n",
    "    axs[2].set_title('Phantast Prediction')\n",
    "    axs[2].axis('off')\n",
    "\n",
    "    axs[3].imshow(model_pred, cmap='gray')\n",
    "    axs[3].set_title(f'{model_name} Prediction')\n",
    "    axs[3].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def visualise_difference_map(ground_truth, prediction, image):\n",
    "   \n",
    "   # Create an empty RGB image\n",
    "   height, width = ground_truth.shape\n",
    "   confusion_image = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "\n",
    "   # Create masks for different scenarios\n",
    "   true_positive_mask = (ground_truth == 1) & (prediction == 1)\n",
    "   true_negative_mask = (ground_truth == 0) & (prediction == 0)\n",
    "   false_positive_mask = (ground_truth == 0) & (prediction == 1)\n",
    "   false_negative_mask = (ground_truth == 1) & (prediction == 0)\n",
    "\n",
    "   # Apply colors\n",
    "   confusion_image[true_positive_mask] = [255, 255, 255]  # White for true positives (cell)\n",
    "   confusion_image[true_negative_mask] = [0, 0, 0]        # Black for true negatives (background)\n",
    "   confusion_image[false_positive_mask] = [0, 0, 255]     # Blue for false positives\n",
    "   confusion_image[false_negative_mask] = [255, 0, 0]     # Red for false negatives\n",
    "\n",
    "   plt.figure(figsize=(18, 6))  \n",
    "  \n",
    "   # Subplot 1: Original Image\n",
    "   plt.subplot(1, 4, 1)\n",
    "   plt.imshow(image, cmap='gray')\n",
    "   plt.title('Original Image', fontsize=25)  # Make font size bigger for report\n",
    "   plt.axis('off')\n",
    "  \n",
    "   # Subplot 2: Ground Truth\n",
    "   plt.subplot(1, 4, 2)\n",
    "   plt.imshow(ground_truth, cmap='gray')\n",
    "   plt.title('Ground Truth', fontsize=25)  \n",
    "   plt.axis('off')\n",
    "  \n",
    "   # Subplot 3: Prediction\n",
    "   plt.subplot(1, 4, 3)\n",
    "   plt.imshow(prediction, cmap='gray')\n",
    "   plt.title('Prediction', fontsize=25) \n",
    "   plt.axis('off')\n",
    "  \n",
    "   # Subplot 4: Difference Map\n",
    "   plt.subplot(1, 4, 4)\n",
    "   plt.imshow(confusion_image)\n",
    "   plt.title('Difference Map', fontsize=25)  \n",
    "   plt.axis('off')\n",
    "  \n",
    "   plt.tight_layout()\n",
    "   plt.show()\n",
    "\n",
    "\n",
    "def calc_recall_score(gt_mask, pred_mask):\n",
    "    intersect = np.sum(pred_mask*gt_mask)\n",
    "    total_pixel_truth = np.sum(gt_mask)\n",
    "    if total_pixel_truth == 0:\n",
    "        return 0\n",
    "    recall = np.mean(intersect/total_pixel_truth)\n",
    "    return round(recall, 6)\n",
    "\n",
    "# Precision = TP / (TP + FP)\n",
    "def calc_precision_score(gt_mask, pred_mask):\n",
    "    intersect = np.sum(pred_mask*gt_mask)\n",
    "    total_pixel_pred = np.sum(pred_mask)\n",
    "    if total_pixel_pred == 0:\n",
    "        return 0\n",
    "    precision = np.mean(intersect/total_pixel_pred)\n",
    "    return round(precision, 6)\n",
    "\n",
    "# Dice = 2TP / (2TP + FP + FN)\n",
    "def calc_dice_coef(gt_mask, pred_mask):\n",
    "    intersect = np.sum(pred_mask*gt_mask)\n",
    "    total_sum = np.sum(pred_mask) + np.sum(gt_mask)\n",
    "    if total_sum == 0:\n",
    "        return 0\n",
    "    dice = np.mean(2*intersect/total_sum)\n",
    "    return round(dice, 6)\n",
    "    \n",
    "def calc_iou(gt_mask, pred_mask):\n",
    "    # Ensure the images are binary\n",
    "    gt_mask = gt_mask > 0.5\n",
    "    pred_mask = pred_mask > 0.5\n",
    "    \n",
    "    intersection = np.logical_and(gt_mask, pred_mask)\n",
    "    union = np.logical_or(gt_mask, pred_mask)\n",
    "    \n",
    "    if np.sum(union) == 0:\n",
    "        return 0\n",
    "    iou = np.sum(intersection) / np.sum(union)\n",
    "    return iou\n",
    "\n",
    "def plot_confusion_matrix(tp, fp, tn, fn, model_name):\n",
    "    # Round values to the nearest integer\n",
    "    tp = round(tp)\n",
    "    fp = round(fp)\n",
    "    tn = round(tn)\n",
    "    fn = round(fn)\n",
    "    \n",
    "    # Create the confusion matrix\n",
    "    confusion_matrix = np.array([[tp, fn],\n",
    "                                 [fp, tn]])\n",
    "    \n",
    "    # Plot\n",
    "    sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Predicted Positive', 'Predicted Negative'],\n",
    "                yticklabels=['Actual Positive', 'Actual Negative'])\n",
    "    plt.title(f'Confusion Matrix for {model_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_side_by_side_images(dir1, dir2):\n",
    "    images1 = natsorted([f for f in os.listdir(dir1) if f.endswith(('.tif', '.tiff', '.png', '.jpg'))])\n",
    "    images2 = natsorted([f for f in os.listdir(dir2) if f.endswith(('.tif', '.tiff', '.png', '.jpg'))])\n",
    "    \n",
    "    assert len(images1) == len(images2), \"Directories contain different amounts of images.\"\n",
    "\n",
    "    for img1, img2 in zip(images1, images2):\n",
    "\n",
    "        image1 = tiff.imread(os.path.join(dir1, img1))\n",
    "        image2 = tiff.imread(os.path.join(dir2, img2))\n",
    "\n",
    "        # Create a figure with two subplots\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "        axs[0].imshow(image1, cmap='gray')\n",
    "        axs[0].set_title(f'Image from {dir1}')\n",
    "        axs[0].axis('off')\n",
    "\n",
    "        axs[1].imshow(image2, cmap='gray')\n",
    "        axs[1].set_title(f'Image from {dir2}')\n",
    "        axs[1].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test indexes (needed to line up images from other models during stack separation and renaming)\n",
    "indexes = [os.path.basename(label).split('_')[-1].split('.')[0] for label in test_gt_paths]\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store model segmentations in 'model_segmentations' dictionary\n",
    "\n",
    "dir_path = \"/vol/biomedic3/bglocker/mscproj24/nma23/data/testing_directory/multi_model/eval_test_data/\"\n",
    "\n",
    "# List of models (each model has a directory in dir_path that was created from)\n",
    "eval_models = ['PHANTAST',\n",
    "                'vit_b', \n",
    "                'vit_b_lm_amg', \n",
    "                'vit_b_lm_ais', \n",
    "                'vit_l', \n",
    "                'vit_l_lm_amg', \n",
    "                'vit_l_lm_ais',\n",
    "                'dilated_vit_l_lm_ais', \n",
    "                'cellpose3', \n",
    "                'ensemble_1', \n",
    "                'ensemble_2', \n",
    "                'ensemble_3', \n",
    "                'ensemble_4_w_PHANTAST', \n",
    "                'ensemble_5_w_PHANTAST',\n",
    "                'finetuned_vit_b',\n",
    "                'finetuned_vit_b_lm',\n",
    "                'finetuned_vit_l',\n",
    "                'finetuned_vit_l_lm'\n",
    "                ]\n",
    "\n",
    "# Dictionary to store the segmentation file paths for each model\n",
    "model_segmentations = {}\n",
    "\n",
    "# Get segmentations for each model and store in dictionary\n",
    "for model in eval_models:\n",
    "    model_dir = os.path.join(dir_path, f\"Model_{model}/Segmentation_Output\")\n",
    "\n",
    "    # Get segmentation file paths for each model\n",
    "    model_segmentations[model] = list_files_in_directory(model_dir)\n",
    "\n",
    "    if len(model_segmentations[model]) != 150:\n",
    "        print(f\"Model {model} has {len(model_segmentations[model])} segmentations\")\n",
    "\n",
    "# Print dictionary keys\n",
    "print(f\"Dictionary 'model_segmentations' keys: {model_segmentations.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot normal and dilated predictions side by side\n",
    "def plot_normal_dilated(normal, dilated):\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    fontsize = 25\n",
    "\n",
    "    axs[0].imshow(normal, cmap='gray')\n",
    "    axs[0].set_title('Normal Prediction', fontsize=fontsize)\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    axs[1].imshow(dilated, cmap='gray')\n",
    "    axs[1].set_title('Dilated Prediction', fontsize=fontsize)\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "normal_image_path = os.path.join(dir_path, \"Model_vit_l_lm_ais/Segmentation_Output\", model_segmentations['vit_l_lm_ais'][4])\n",
    "dilated_image_path = os.path.join(dir_path, \"Model_dilated_vit_l_lm_ais/Segmentation_Output\", model_segmentations['dilated_vit_l_lm_ais'][4])\n",
    "\n",
    "normal_image = tiff.imread(normal_image_path)\n",
    "dilated_image = tiff.imread(dilated_image_path)\n",
    "\n",
    "plot_normal_dilated(normal_image, dilated_image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for all models\n",
    "results_list = []\n",
    "\n",
    "for model in eval_models:\n",
    "    print(f\"Running metrics evaluation for {model}\")  \n",
    "\n",
    "    # Initialize totals to 0\n",
    "    tp_total = 0\n",
    "    fp_total = 0\n",
    "    tn_total = 0\n",
    "    fn_total = 0\n",
    "    count = len(model_segmentations[model])\n",
    "\n",
    "    # Loop to calculate metrics for each image\n",
    "    for i in range(count):\n",
    "        # Load the corresponding ground truth and predicted segmentation\n",
    "        test_gt = tiff.imread(os.path.join(test_gt_normalised_mask_dir, gt_relative_filepaths[i]))\n",
    "        test_pred = tiff.imread(model_segmentations[model][i])\n",
    "\n",
    "        # Calculate confusion matrix values\n",
    "        tp = np.sum((test_gt == 1) & (test_pred == 1))\n",
    "        fp = np.sum((test_gt == 0) & (test_pred == 1))\n",
    "        tn = np.sum((test_gt == 0) & (test_pred == 0))\n",
    "        fn = np.sum((test_gt == 1) & (test_pred == 0))\n",
    "\n",
    "        # Aggregate the confusion matrix values\n",
    "        tp_total += tp\n",
    "        fp_total += fp\n",
    "        tn_total += tn\n",
    "        fn_total += fn\n",
    "        \n",
    "        # Calculate recall, precision, dice, and IoU\n",
    "        recall = calc_recall_score(test_gt, test_pred)\n",
    "        precision = calc_precision_score(test_gt, test_pred)\n",
    "        dice = calc_dice_coef(test_gt, test_pred)\n",
    "        iou = calc_iou(test_gt, test_pred)\n",
    "\n",
    "        # Store metrics for each image in results list\n",
    "        results_list.append({\n",
    "            \"Model\": model,\n",
    "            \"Recall\": recall,\n",
    "            \"Precision\": precision,\n",
    "            \"Dice Coefficient\": dice,\n",
    "            \"IoU\": iou,\n",
    "            \"True Positives\": tp,\n",
    "            \"False Positives\": fp,\n",
    "            \"True Negatives\": tn,\n",
    "            \"False Negatives\": fn\n",
    "        })\n",
    "\n",
    "    # After all images, store total confusion matrix values\n",
    "    results_list.append({\n",
    "        \"Model\": model,\n",
    "        \"True Positives\": tp_total,\n",
    "        \"False Positives\": fp_total,\n",
    "        \"True Negatives\": tn_total,\n",
    "        \"False Negatives\": fn_total\n",
    "    })\n",
    "\n",
    "# Convert list to DataFrame\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Create a DataFrame for general metrics (Recall, Precision, Dice, IoU)\n",
    "general_metrics_df = results_df[[\"Model\", \"Recall\", \"Precision\", \"Dice Coefficient\", \"IoU\"]].dropna()\n",
    "general_metrics_df = general_metrics_df.groupby(\"Model\").mean().reset_index()\n",
    "\n",
    "# Create a DataFrame for TP, FP, TN, FN\n",
    "tp_fp_tn_fn_df = results_df[[\"Model\", \"True Positives\", \"False Positives\", \"True Negatives\", \"False Negatives\"]].dropna()\n",
    "tp_fp_tn_fn_df = tp_fp_tn_fn_df.groupby(\"Model\").sum().reset_index()  # Ensure these are summed, not averaged\n",
    "\n",
    "# Sort and display the general metrics DataFrame by Dice Coefficient in descending order\n",
    "general_metrics_df = general_metrics_df.sort_values(by=\"Dice Coefficient\", ascending=False)\n",
    "print(\"Average Metrics for Each Model, Sorted by Dice Coefficient\")\n",
    "display(general_metrics_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, just looking at models of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models_of_interest list\n",
    "\n",
    "models_of_interest = [    \n",
    "    'finetuned_vit_l', \n",
    "    'finetuned_vit_l_lm', \n",
    "    'finetuned_vit_b_lm', \n",
    "    'finetuned_vit_b', \n",
    "    'PHANTAST', \n",
    "    'cellpose3'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion metrics\n",
    "\n",
    "# Filter the results DataFrame for the models of interest\n",
    "filtered_results_df = tp_fp_tn_fn_df[tp_fp_tn_fn_df['Model'].isin(models_of_interest)]\n",
    "\n",
    "# Calculate and display FP/FN ratios for each model in the models of interest\n",
    "filtered_results_df[\"FP/FN Ratio\"] = filtered_results_df[\"False Positives\"] / filtered_results_df[\"False Negatives\"]\n",
    "\n",
    "# Display the TP, FP, TN, FN counts and FP/FN ratios\n",
    "print(\"Total TP, FP, TN, FN for Models of Interest\")\n",
    "display(filtered_results_df)\n",
    "\n",
    "print(\"False Positives to False Negatives Ratios for Models of Interest\")\n",
    "display(filtered_results_df[[\"Model\", \"FP/FN Ratio\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot difference maps between ground truth and prediction masks\n",
    "\n",
    "# pred_dir = '/vol/biomedic3/bglocker/mscproj24/nma23/data/testing_directory/multi_model/eval_test_data/Model_finetuned_vit_l_lm/Segmentation_Output'\n",
    "pred_dir = \"/vol/biomedic3/bglocker/mscproj24/nma23/data/testing_directory/multi_model/eval_test_data/Model_finetuned_vit_b_lm/Segmentation_Output\"\n",
    "\n",
    "pred_abs_files = natsorted(os.listdir(pred_dir))\n",
    "pred_files = [os.path.join(pred_dir, f) for f in pred_abs_files]\n",
    "\n",
    "# print(f\"gt_files: {gt_files}\")\n",
    "# print(f\"pred_files: {pred_files}\")\n",
    "# print(f\"image_files: {image_files}\")\n",
    "# print dtypes\n",
    "\n",
    "\n",
    "# for idx, (gt_file, pred_file, image_file) in enumerate(zip(gt_files, pred_files, image_files)):\n",
    "#    if idx >3:  # Limit the number of images to display\n",
    "#       break\n",
    "#    # Load the ground truth and prediction masks\n",
    "#    gt_mask = tiff.imread(gt_file)\n",
    "#    pred_mask = tiff.imread(pred_file)\n",
    "#    image = tiff.imread(image_file)\n",
    "\n",
    "#    # May need to convert masks to 0-1 range depending on where they're loaded from\n",
    "#    gt_mask = gt_mask / np.max(gt_mask)\n",
    "#    gt_mask = gt_mask.astype(np.uint8)\n",
    "\n",
    "#    print(f\"gt_mask: {gt_file}\")\n",
    "#    print(f\"pred_mask: {pred_file}\")\n",
    "#    print(f\"image: {image_file}\")\n",
    "\n",
    "#    print(f\"gt_files dtype: {type(gt_file)}, min: {np.min(gt_mask)}, max: {np.max(gt_mask)}\")\n",
    "#    print(f\"pred_files dtype: {type(pred_file)}, min: {np.min(pred_mask)}, max: {np.max(pred_mask)}\")\n",
    "  \n",
    "#    # Visualize the difference map\n",
    "#    visualise_difference_map(gt_mask, pred_mask, image)\n",
    "\n",
    "# Assuming gt_files, pred_files, and image_files are already defined and have the same length\n",
    "\n",
    "index = 60\n",
    "\n",
    "gt_file = gt_files[index]\n",
    "pred_file = pred_files[index]\n",
    "image_file = image_files[index]\n",
    "\n",
    "gt_mask = tiff.imread(gt_file)\n",
    "pred_mask = tiff.imread(pred_file)\n",
    "image = tiff.imread(image_file)\n",
    "\n",
    "# Print file paths for debugging\n",
    "print(f\"GT File: {gt_file}\")\n",
    "print(f\"Pred File: {pred_file}\")\n",
    "print(f\"Image File: {image_file}\")\n",
    "\n",
    "# Visualize the difference map for the 56th image\n",
    "visualise_difference_map(gt_mask, pred_mask, image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find images where PHANTAST outperforms a given model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find images where PHANTAST outperforms a given model\n",
    "\n",
    "dice_comparison_data = []\n",
    "win_counts_data = []\n",
    "\n",
    "# Calculate PHANTAST Dice Coefficients for each image\n",
    "phantast_dice = []\n",
    "phantast_masks = []  \n",
    "for i in range(len(gt_files)):\n",
    "    gt_mask = tiff.imread(gt_files[i])\n",
    "    phantast_mask = tiff.imread(model_segmentations['PHANTAST'][i])\n",
    "    dice_value = calc_dice_coef(gt_mask, phantast_mask)\n",
    "    phantast_dice.append(dice_value)\n",
    "    phantast_masks.append(phantast_mask)\n",
    "\n",
    "    dice_comparison_data.append({'Model': 'PHANTAST', 'Image_Index': i, 'Dice_Coefficient': dice_value})\n",
    "\n",
    "# Loop through each model and compare with PHANTAST\n",
    "for model in models_of_interest:\n",
    "\n",
    "    if model == 'PHANTAST' or model == 'cellpose3':\n",
    "        continue\n",
    "\n",
    "    phantast_win = 0\n",
    "    model_win = 0\n",
    "\n",
    "    for i in range(len(gt_files)):\n",
    "        gt_mask = tiff.imread(gt_files[i])\n",
    "        model_mask = tiff.imread(model_segmentations[model][i])\n",
    "\n",
    "        model_dice = calc_dice_coef(gt_mask, model_mask)\n",
    "\n",
    "        # Append model's Dice Coefficient to the comparison data\n",
    "        dice_comparison_data.append({'Model': model, 'Image_Index': i, 'Dice_Coefficient': model_dice})\n",
    "\n",
    "        # Compare PHANTAST and the current model for this image\n",
    "        if phantast_dice[i] > model_dice:\n",
    "\n",
    "            print(f\"PHANTAST outperforms {model} on image {i} with Dice Coefficient {phantast_dice[i]} > {model_dice}\")\n",
    "\n",
    "            # Plot the masks\n",
    "            # plot_samples(gt_mask, phantast_masks[i]) \n",
    "            # plot_samples(gt_mask, model_mask)\n",
    "            plot_four_images(tiff.imread(image_files[i]), gt_mask, phantast_masks[i], model_mask, model)\n",
    "            phantast_win += 1\n",
    "        elif model_dice > phantast_dice[i]:\n",
    "            model_win += 1\n",
    "        else:\n",
    "            print(f\"PHANTAST and {model} have the same Dice Coefficient of {phantast_dice[i]}\")\n",
    "\n",
    "    # Store win counts\n",
    "    win_counts_data.append({'Model': model, 'PHANTAST_Wins': phantast_win, 'Model_Wins': model_win})\n",
    "\n",
    "# Convert to DataFrames\n",
    "dice_comparison_df = pd.DataFrame(dice_comparison_data)\n",
    "win_counts_df = pd.DataFrame(win_counts_data)\n",
    "\n",
    "# Display the win counts\n",
    "print(\"Win Counts Out of 150 Images\")\n",
    "for model in models_of_interest:\n",
    "    if model == 'PHANTAST' or model == 'cellpose3':\n",
    "        continue\n",
    "    model_data = win_counts_df[win_counts_df['Model'] == model].iloc[0]\n",
    "    print(f\"Win Counts for {model}:\")\n",
    "    print(f\"PHANTAST Wins: {model_data['PHANTAST_Wins']}\")\n",
    "    print(f\"{model} Wins: {model_data['Model_Wins']}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot violin plots of Dice Coefficient for PHANTAST vs models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot violin plots of Dice coefficient for PHANTAST vs models \n",
    "\n",
    "# NOTE: where the plot is wider, there is more data at that value\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.violinplot(data=dice_comparison_df, x='Model', y='Dice_Coefficient')\n",
    "plt.title('Dice Coefficient Comparison Between PHANTAST and Finetuned Models')\n",
    "plt.ylabel('Dice Coefficient')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Recall  Precision\n",
      "25th Percentile (Q1)      0.249483   0.110745\n",
      "50th Percentile (Median)  0.999499   0.546912\n",
      "75th Percentile (Q3)      0.999910   0.700654\n",
      "IQR                       0.750427   0.589908\n"
     ]
    }
   ],
   "source": [
    "# Disk performance\n",
    "\n",
    "dir1 = \"/vol/biomedic3/bglocker/mscproj24/nma23/data/testing_directory/multi_model/eval_test_data/inhomogeneous_light_exps/inhomogeneous_light_norm_labels\"\n",
    "dir2 = \"/vol/biomedic3/bglocker/mscproj24/nma23/data/testing_directory/multi_model/eval_test_data/inhomogeneous_light_exps/disk30_finetuned_vit_l_lm/Segmentation_Output\"\n",
    "\n",
    "\n",
    "images1 = natsorted([f for f in os.listdir(dir1) if f.endswith(('.tif', '.tiff', '.png', '.jpg'))])\n",
    "images2 = natsorted([f for f in os.listdir(dir2) if f.endswith(('.tif', '.tiff', '.png', '.jpg'))])\n",
    "\n",
    "recall_scores = []\n",
    "precision_scores = []\n",
    "\n",
    "# Loop through both image lists, calculate recall and precision\n",
    "for img1, img2 in zip(images1, images2):\n",
    "    # Load the images\n",
    "    image1 = tiff.imread(os.path.join(dir1, img1))\n",
    "    image2 = tiff.imread(os.path.join(dir2, img2))\n",
    "\n",
    "    recall = calc_recall_score(image1, image2)\n",
    "    precision = calc_precision_score(image1, image2)\n",
    "    \n",
    "    recall_scores.append(recall)\n",
    "    precision_scores.append(precision)\n",
    "\n",
    "# Convert scores to numpy arrays\n",
    "recall_scores = np.array(recall_scores)\n",
    "precision_scores = np.array(precision_scores)\n",
    "\n",
    "# Calculate the quartiles for recall and precision scores\n",
    "recall_iqr = np.percentile(recall_scores, [25, 50, 75])\n",
    "precision_iqr = np.percentile(precision_scores, [25, 50, 75])\n",
    "\n",
    "# Store the IQR data for both metrics\n",
    "iqr_data = {\n",
    "    \"Recall\": {\n",
    "        \"25th Percentile (Q1)\": recall_iqr[0],\n",
    "        \"50th Percentile (Median)\": recall_iqr[1],\n",
    "        \"75th Percentile (Q3)\": recall_iqr[2],\n",
    "        \"IQR\": recall_iqr[2] - recall_iqr[0]\n",
    "    },\n",
    "    \"Precision\": {\n",
    "        \"25th Percentile (Q1)\": precision_iqr[0],\n",
    "        \"50th Percentile (Median)\": precision_iqr[1],\n",
    "        \"75th Percentile (Q3)\": precision_iqr[2],\n",
    "        \"IQR\": precision_iqr[2] - precision_iqr[0]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create a DataFrame with the IQR data\n",
    "iqr_df = pd.DataFrame(iqr_data)\n",
    "\n",
    "print(iqr_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micro-sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
