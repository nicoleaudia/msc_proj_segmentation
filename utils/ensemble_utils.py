from natsort import natsorted
import os
from glob import glob
import numpy as np

from cellpose import denoise

from utils.exp_manipulation_utils import read_tiff_images_from_dir
from utils.stack_manipulation_utils import normalise_ml_stack, save_images_as_stack, save_indv_images
from utils.phantast_utils import run_phantast_headless, run_phantast_interactive, normalise_phantast_save_images_in_dir
from utils.microsam_utils import process_images_in_batches

def ensemble_process(models, running_in_headless_mode, bf_dir, segmentation_dir, ensemble_dir, acceptable_file_types, ij, phantast_headless_source_dir=None, phantast_headless_destination_dir=None):
    """
    Perform segmentation with various available models and ensemble prediction (PHANTAST, micro_sam, cellpose, etc.).

    Args:
        models (list): List of model names to be used in the ensemble.
        running_in_headless_mode (bool): Flag indicating if the script is running in headless mode.
        bf_dir (str): Directory containing input images.
        segmentation_dir (str): Directory where segmentation results will be stored.
        ensemble_dir (str): Directory where ensemble results will be stored.
        acceptable_file_types (str): File type filter for reading images from bf_dir (e.g., ".tiff"); can pass in a tuple.
        ij: Fiji/ImageJ instance for running PHANTAST in interactive mode.
        phantast_headless_source_dir (str, optional): Source directory for PHANTAST images in headless mode.
        phantast_headless_destination_dir (str, optional): Destination directory for PHANTAST images in headless mode.

    Returns:
        final_masks: List of final segmentation masks generated by averaging the predictions of the models in the ensemble.

    Raises:
        ValueError: If headless mode is enabled and PHANTAST directories are not provided.
    """
    # Dictionary to store segmentations for all models in ensemble
    all_segmentations = {}

    # Run each model
    for indv_model in models:
        filenames = natsorted([f for f in os.listdir(bf_dir) if f.endswith(acceptable_file_types)])
        batch_size = 3
        all_segmentations[indv_model] = []

        if indv_model == "PHANTAST":
            if running_in_headless_mode:
                # Check if the directories are provided
                if phantast_headless_source_dir is None or phantast_headless_destination_dir is None:
                    raise ValueError("Headless mode is enabled, but phantast_headless_source_dir or phantast_headless_destination_dir is not provided.")
                run_phantast_headless(phantast_headless_source_dir, phantast_headless_destination_dir, running_in_headless_mode)
                
                headless_phantast_images = read_tiff_images_from_dir(phantast_headless_destination_dir)
                # Append all_segmentations with PHANTAST results
                all_segmentations[indv_model] = headless_phantast_images

            else:
                run_phantast_interactive(running_in_headless_mode, bf_dir=bf_dir, phantast_dir=segmentation_dir, ij=ij)
                # Normalise PHANTAST images (/255), invert, replace in directory, and return as stack. Append all_segmentations with PHANTAST results
                all_segmentations[indv_model] = normalise_phantast_save_images_in_dir(segmentation_dir)

            # Delete PHANTAST images in segmentation_dir to avoid shape issues (okay because they are saved to all_segmentations[PHANTAST])
            delete_files_with_prefix(segmentation_dir, "PHANTAST")

        elif indv_model.startswith("vit_"):

            # Identify model and usam_lm_algorithm, if applicable
            if indv_model.endswith("amg"):
                usam_lm_algorithm = "amg"
                indv_model_id = indv_model[:-4] # Remove "_amg" from end of string
            elif indv_model.endswith("ais"):
                usam_lm_algorithm = "ais"
                indv_model_id = indv_model[:-4] # Remove "_ais" from end of string
            else:
                usam_lm_algorithm = None
                indv_model_id = indv_model

            # Perform segmentation via micro_sam
            all_segmentations[indv_model] = process_images_in_batches(
                filenames=filenames, 
                batch_size=batch_size, 
                bf_dir=bf_dir, 
                model_choice=indv_model_id, 
                usam_lm_algorithm=usam_lm_algorithm
                )
            
        elif indv_model.startswith("cellpose"):
            imgs = read_tiff_images_from_dir(bf_dir)
            channels = [0,0] 
            diams = 50
            cellpose3_model = denoise.CellposeDenoiseModel(gpu=True, model_type="cyto3", restore_type="denoise_cyto3")
            masks, flows, styles, imgs_dn = cellpose3_model.eval(imgs, diameter=diams, channels=channels)
            all_segmentations[indv_model] = normalise_ml_stack(masks)

    for model_name, images in all_segmentations.items():
        output_filename = f'{model_name}_segmentations_stack.tiff'
        try:
            save_images_as_stack(ensemble_dir, images, output_filename)
        except Exception as e:
            print(f"Failed to save {model_name} stack: {e}")

    num_images = len(next(iter(all_segmentations.values())))  # Get the number of images from any model's segmentations

    final_masks = []

    # Take simple average and create new masks
    for i in range(num_images):
        segmentations_for_image = [all_segmentations[m][i] for m in models]
        final_mask = simple_avg_ensemble(segmentations_for_image)
        final_masks.append(final_mask)
    
    save_indv_images(final_masks, segmentation_dir, "image")   
    
    return final_masks


def simple_avg_ensemble(all_segmentations):
    """
    Compute the average of multiple segmentation predictions and binarise the result to produce a final segmentation mask.

    Args:
        all_segmentations (list of numpy arrays): List of 2D arrays representing segmentation results from different models.

    Returns:
        numpy.ndarray: Final segmentation mask stack after averaging and thresholding (0.5 threshold is used).

    """
    # Stack predictions along a new axis
    stacked_preds = np.stack(all_segmentations, axis=0)
    
    # Compute the average prediction
    print(f"stacked preds shape: {stacked_preds.shape}")
    print(f"stacked preds dtype: {stacked_preds.dtype}")
    # print(stacked_preds)
    average_pred = np.mean(stacked_preds, axis=0)

    print(f"average_pred min: {np.min(average_pred)}, max: {np.max(average_pred)}")
    # print(f"average_pred: {average_pred}")
    
    # Binarize the average prediction to get final segmentation mask
    final_masks = (average_pred > 0.5).astype(np.uint8) # Use threshold of 0.5 to represent "more than half" of models supporting that point
    
    print(f"final_masks min: {np.min(final_masks)}, max: {np.max(final_masks)}")
    # print(final_masks)

    return final_masks


def delete_files_with_prefix(directory, prefix):
    """
    Delete all files in a directory that start with a given prefix.

    Args:
        directory (str): The directory where files are located.
        prefix (str): The prefix used to identify files for deletion.

    Returns:
        None: Deletes the files directly from the filesystem.
        
    Raises:
        OSError: If there is an issue removing a file.
    """
    # Find all files matching the pattern
    search_pattern = os.path.join(directory, f"{prefix}*")
    files_to_delete = glob(search_pattern)
    
    # Delete each file
    for file_path in files_to_delete:
        try:
            os.remove(file_path)
            print(f"Deleted: {file_path}")
        except Exception as e:
            print(f"Error deleting {file_path}: {e}")